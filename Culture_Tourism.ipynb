{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXEYfFquAv5_",
        "outputId": "d6c48201-a7e6-4adb-9a06-c46976188906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.51)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.51)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (2.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (4.13.1)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain-groq langchain chromadb sentence-transformers unstructured\n",
        "!pip install -U langchain-community\n",
        "!pip install -U langchain\n",
        "!pip install -U httpx\n",
        "!pip install groq\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Cyo7Uv6gExp"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "import chromadb  # Add this import\n",
        "\n",
        "# Setup for Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(api_key=groq_api_key, model_name=\"llama3-8b-8192\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYduWkW7BxJ-",
        "outputId": "ec2637a1-a539-49ce-8755-ba233f561acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kULK7kZJB4pQ",
        "outputId": "f820001d-88a0-45ff-f75a-66cf5b929153"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!pip install pypdf\\n!pip install -U langchain-community'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''!pip install pypdf\n",
        "!pip install -U langchain-community'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "PVYiPS72CA5p",
        "outputId": "04a2d0ca-98e9-430c-d680-545d5ffec481"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from langchain.vectorstores import Chroma\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.document_loaders import PyPDFLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport os\\n\\n# Setup\\npdf_folder_path = \"/content/drive/MyDrive/Culture_and_Quiz\"\\npersist_directory = \"/content/drive/MyDrive/vector/Culture_and_Quiz2_db\"\\nembedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n\\n# Load and process all PDFs in folder\\nall_docs = []\\nfor filename in os.listdir(pdf_folder_path):\\n    if filename.endswith(\".pdf\"):\\n        pdf_path = os.path.join(pdf_folder_path, filename)\\n        loader = PyPDFLoader(pdf_path)\\n        documents = loader.load()\\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\n        chunks = splitter.split_documents(documents)\\n        all_docs.extend(chunks)\\n\\n# Check if we loaded anything\\nprint(f\"âœ… Total chunks: {len(all_docs)}\")\\n\\n# Store in Chroma\\nvectorstore = Chroma.from_documents(\\n    documents=all_docs,\\n    embedding=embedding,\\n    persist_directory=persist_directory\\n)\\nvectorstore.persist()\\nprint(\"âœ… Vector DB created and persisted.\")'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "# Setup\n",
        "pdf_folder_path = \"/content/drive/MyDrive/Culture_and_Quiz\"\n",
        "persist_directory = \"/content/drive/MyDrive/vector/Culture_and_Quiz2_db\"\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load and process all PDFs in folder\n",
        "all_docs = []\n",
        "for filename in os.listdir(pdf_folder_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents = loader.load()\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        chunks = splitter.split_documents(documents)\n",
        "        all_docs.extend(chunks)\n",
        "\n",
        "# Check if we loaded anything\n",
        "print(f\"âœ… Total chunks: {len(all_docs)}\")\n",
        "\n",
        "# Store in Chroma\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_docs,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "vectorstore.persist()\n",
        "print(\"âœ… Vector DB created and persisted.\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dHCiyG4fFu0o",
        "outputId": "1c2ecbde-b905-42a1-f0fa-31f1d4783fd3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from langchain.vectorstores import Chroma\\n\\nvectorstore = Chroma(\\n    persist_directory=persist_directory,\\n    embedding_function=embedding\\n)\\n\\nretriever = vectorstore.as_retriever()\\n\\n# Print few documents\\nfor i, doc in enumerate(vectorstore._collection.get()[\\'documents\\'][:8]):\\n    print(f\"\\nðŸ“„ Doc {i+1}:\\n{doc[:500]}...\")'"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Print few documents\n",
        "for i, doc in enumerate(vectorstore._collection.get()['documents'][:8]):\n",
        "    print(f\"\\nðŸ“„ Doc {i+1}:\\n{doc[:500]}...\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "LPoi_NHVOHA-",
        "outputId": "e555f291-9cd5-477a-80d9-6c74db97d97a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from langchain.vectorstores import Chroma\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.document_loaders import PyPDFLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport os\\n\\n# === Configuration ===\\nstory_pdf_folder_path = \"/content/drive/MyDrive/Indian_Stories\"  # Folder with story PDFs\\nstory_vector_db_path = \"/content/drive/MyDrive/vector/Indian_Stories_db\"  # Persistence path\\n\\n# === Load embedding model ===\\nembedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n\\n# === Load and process story documents ===\\nall_story_docs = []\\nfor filename in os.listdir(story_pdf_folder_path):\\n    if filename.endswith(\".pdf\"):\\n        pdf_path = os.path.join(story_pdf_folder_path, filename)\\n        loader = PyPDFLoader(pdf_path)\\n        documents = loader.load()\\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\n        chunks = splitter.split_documents(documents)\\n        all_story_docs.extend(chunks)\\n\\nprint(f\"âœ… Total story chunks: {len(all_story_docs)}\")\\n\\n# === Create and persist vector store for stories ===\\nstory_vectorstore = Chroma.from_documents(\\n    documents=all_story_docs,\\n    embedding=embedding,\\n    persist_directory=story_vector_db_path\\n)\\nstory_vectorstore.persist()\\nprint(\"âœ… Story vector DB created and saved to disk.\")'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "# === Configuration ===\n",
        "story_pdf_folder_path = \"/content/drive/MyDrive/Indian_Stories\"  # Folder with story PDFs\n",
        "story_vector_db_path = \"/content/drive/MyDrive/vector/Indian_Stories_db\"  # Persistence path\n",
        "\n",
        "# === Load embedding model ===\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# === Load and process story documents ===\n",
        "all_story_docs = []\n",
        "for filename in os.listdir(story_pdf_folder_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(story_pdf_folder_path, filename)\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents = loader.load()\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        chunks = splitter.split_documents(documents)\n",
        "        all_story_docs.extend(chunks)\n",
        "\n",
        "print(f\"âœ… Total story chunks: {len(all_story_docs)}\")\n",
        "\n",
        "# === Create and persist vector store for stories ===\n",
        "story_vectorstore = Chroma.from_documents(\n",
        "    documents=all_story_docs,\n",
        "    embedding=embedding,\n",
        "    persist_directory=story_vector_db_path\n",
        ")\n",
        "story_vectorstore.persist()\n",
        "print(\"âœ… Story vector DB created and saved to disk.\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "kUP1Qg2GQNgJ",
        "outputId": "ee28176b-b933-456f-a66f-eaff1bd38095"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from langchain.vectorstores import Chroma\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n# === Load embedding model ===\\nembedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n\\n# === Load existing Chroma vector DB for stories ===\\nstories_persist_directory = \"/content/drive/MyDrive/vector/Indian_Stories_db\"\\n\\n# === Initialize vector store ===\\nstories_vectorstore = Chroma(\\n    persist_directory=stories_persist_directory,\\n    embedding_function=embedding\\n)\\n\\n# === Create retriever ===\\nstories_retriever = stories_vectorstore.as_retriever()\\n\\n# === Retrieve and print a few documents for preview ===\\nsample_docs = stories_retriever.get_relevant_documents(\"Show me a story\")  # dummy query\\n\\nif not sample_docs:\\n    print(\"âš ï¸ No documents retrieved. Try re-checking the DB content or query.\")\\nelse:\\n    for i, doc in enumerate(sample_docs[:5]):  # print first 5\\n        print(f\"\\nðŸ“„ Story Doc {i+1}:\")\\n        print(f\"ðŸ“š Content Preview:\\n{doc.page_content[:500]}...\")\\n        print(f\"ðŸ“ Metadata: {doc.metadata}\")'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# === Load embedding model ===\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# === Load existing Chroma vector DB for stories ===\n",
        "stories_persist_directory = \"/content/drive/MyDrive/vector/Indian_Stories_db\"\n",
        "\n",
        "# === Initialize vector store ===\n",
        "stories_vectorstore = Chroma(\n",
        "    persist_directory=stories_persist_directory,\n",
        "    embedding_function=embedding\n",
        ")\n",
        "\n",
        "# === Create retriever ===\n",
        "stories_retriever = stories_vectorstore.as_retriever()\n",
        "\n",
        "# === Retrieve and print a few documents for preview ===\n",
        "sample_docs = stories_retriever.get_relevant_documents(\"Show me a story\")  # dummy query\n",
        "\n",
        "if not sample_docs:\n",
        "    print(\"âš ï¸ No documents retrieved. Try re-checking the DB content or query.\")\n",
        "else:\n",
        "    for i, doc in enumerate(sample_docs[:5]):  # print first 5\n",
        "        print(f\"\\nðŸ“„ Story Doc {i+1}:\")\n",
        "        print(f\"ðŸ“š Content Preview:\\n{doc.page_content[:500]}...\")\n",
        "        print(f\"ðŸ“ Metadata: {doc.metadata}\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgdzgjTzpxuR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "a3tNuuq1VmiP",
        "outputId": "2090994f-f52d-4ea4-c385-d6b0cc43c7dd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!pip install chromadb\\nimport chromadb\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport re\\nimport json\\n\\nclass ChromaDBStore:\\n    def __init__(self, persist_dir=\"/content/drive/MyDrive/vector/Tourism_db\"):\\n        self.client = chromadb.PersistentClient(path=persist_dir)\\n        self.collection = self.client.get_or_create_collection(\\n            name=\"indian_tourism\",\\n            metadata={\"hnsw:space\": \"cosine\"}\\n        )\\n\\n    def store_embeddings(self, chunks, embeddings, metadatas=None):\\n\\n        embeddings_list = embeddings.tolist()\\n        ids = [f\"id_{i}\" for i in range(len(chunks))]\\n\\n\\n        if metadatas is None:\\n            metadatas = [{} for _ in chunks]\\n\\n\\n        safe_metadatas = []\\n        for meta in metadatas:\\n            safe_meta = {}\\n            for k, v in meta.items():\\n                if isinstance(v, (list, dict)):\\n\\n                    safe_meta[k] = json.dumps(v)\\n                elif isinstance(v, (str, int, float, bool)):\\n                    safe_meta[k] = v\\n                else:\\n\\n                    safe_meta[k] = str(v)\\n            safe_metadatas.append(safe_meta)\\n\\n\\n        batch_size = 100\\n        with tqdm(total=len(ids), desc=\"Storing in ChromaDB\") as pbar:\\n            for i in range(0, len(ids), batch_size):\\n                batch = {\\n                    \"ids\": ids[i:i + batch_size],\\n                    \"embeddings\": embeddings_list[i:i + batch_size],\\n                    \"documents\": [str(chunk) for chunk in chunks[i:i + batch_size]],\\n                    \"metadatas\": safe_metadatas[i:i + batch_size]\\n                }\\n\\n\\n                self._validate_batch(batch)\\n                self.collection.add(**batch)\\n                pbar.update(len(batch[\"ids\"]))\\n\\n        print(f\"Successfully stored {len(ids)} embeddings\")\\n\\n    def _validate_batch(self, batch):\\n        \"\"\"Strict validation for ChromaDB requirements\"\"\"\\n        required_fields = [\"ids\", \"embeddings\", \"documents\"]\\n        for field in required_fields:\\n            if field not in batch:\\n                raise ValueError(f\"Missing required field: {field}\")\\n\\n            if not isinstance(batch[field], list):\\n                raise ValueError(f\"{field} must be a list\")\\n\\n\\n        base_length = len(batch[\"ids\"])\\n        for field in [\"embeddings\", \"documents\"]:\\n            if len(batch[field]) != base_length:\\n                raise ValueError(f\"Length mismatch: ids ({base_length}) vs {field} ({len(batch[field])})\")\\n\\n\\n        if \"metadatas\" in batch:\\n            if len(batch[\"metadatas\"]) != base_length:\\n                raise ValueError(f\"Metadata length mismatch: expected {base_length}, got {len(batch[\\'metadatas\\'])}\")\\n\\n            for meta in batch[\"metadatas\"]:\\n                if not isinstance(meta, dict):\\n                    raise ValueError(\"Each metadata must be a dictionary\")\\n                for k, v in meta.items():\\n                    if not isinstance(v, (str, int, float, bool)):\\n                        raise ValueError(f\"Metadata value {k} must be str/int/float/bool, got {type(v)}\")\\n\\n    def generate_metadata(self, chunks):\\n        metadatas = []\\n        for chunk in chunks:\\n            try:\\n                chunk = str(chunk)\\n\\n\\n                locations = [word for word in re.findall(r\\'\\x08[A-Z][a-z]+\\x08\\', chunk)\\n                          if word not in [\\'India\\', \\'Indian\\', \\'The\\', \\'You\\']\\n                          and len(word) > 2][:3]\\n                locations_str = \", \".join(locations)\\n\\n\\n                activities = []\\n                activity_map = {\\n                    \\'heritage\\': [\\'fort\\', \\'palace\\', \\'museum\\', \\'temple\\'],\\n                    \\'adventure\\': [\\'trek\\', \\'hike\\', \\'camp\\', \\'rafting\\'],\\n                    \\'spiritual\\': [\\'ashram\\', \\'meditation\\', \\'yoga\\'],\\n                    \\'beach\\': [\\'beach\\', \\'sunset\\', \\'wave\\', \\'coast\\']\\n                }\\n                for activity, keywords in activity_map.items():\\n                    if any(kw in chunk.lower() for kw in keywords):\\n                        activities.append(activity)\\n                activities_str = \", \".join(activities)\\n\\n                metadatas.append({\\n                    \"locations\": locations_str,\\n                    \"activities\": activities_str,\\n                    \"word_count\": len(chunk.split())\\n                })\\n            except Exception as e:\\n                print(f\" Metadata generation error: {str(e)[:100]}...\")\\n                metadatas.append({\\n                    \"locations\": \"\",\\n                    \"activities\": \"\",\\n                    \"word_count\": 0\\n                })\\n        return metadatas\\n\\n\\nif __name__ == \"__main__\":\\n\\n    chunks = np.load(\"/content/drive/MyDrive/Tourism/chunks.npy\", allow_pickle=True)\\n    embeddings = np.load(\"/content/drive/MyDrive/Tourism/embeddings.npy\")\\n\\n\\n\\n    chroma_store = ChromaDBStore()\\n\\n\\n    print(\"Generating ChromaDB-compatible metadata...\")\\n    metadatas = chroma_store.generate_metadata(chunks)\\n\\n\\n    try:\\n        chroma_store.store_embeddings(chunks, embeddings, metadatas)\\n    except Exception as e:\\n        print(f\"Critical error: {e}\")\\n        print(\"Debugging tips:\")\\n        print(\"- Check for None values in chunks\")\\n        print(\"- Verify embeddings are finite numbers\")\\n        print(\"- Inspect first 5 metadata items:\")\\n        for i, meta in enumerate(metadatas[:5]):\\n            print(f\"  {i}: {meta}\")'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''!pip install chromadb\n",
        "import chromadb\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "\n",
        "class ChromaDBStore:\n",
        "    def __init__(self, persist_dir=\"/content/drive/MyDrive/vector/Tourism_db\"):\n",
        "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=\"indian_tourism\",\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "    def store_embeddings(self, chunks, embeddings, metadatas=None):\n",
        "\n",
        "        embeddings_list = embeddings.tolist()\n",
        "        ids = [f\"id_{i}\" for i in range(len(chunks))]\n",
        "\n",
        "\n",
        "        if metadatas is None:\n",
        "            metadatas = [{} for _ in chunks]\n",
        "\n",
        "\n",
        "        safe_metadatas = []\n",
        "        for meta in metadatas:\n",
        "            safe_meta = {}\n",
        "            for k, v in meta.items():\n",
        "                if isinstance(v, (list, dict)):\n",
        "\n",
        "                    safe_meta[k] = json.dumps(v)\n",
        "                elif isinstance(v, (str, int, float, bool)):\n",
        "                    safe_meta[k] = v\n",
        "                else:\n",
        "\n",
        "                    safe_meta[k] = str(v)\n",
        "            safe_metadatas.append(safe_meta)\n",
        "\n",
        "\n",
        "        batch_size = 100\n",
        "        with tqdm(total=len(ids), desc=\"Storing in ChromaDB\") as pbar:\n",
        "            for i in range(0, len(ids), batch_size):\n",
        "                batch = {\n",
        "                    \"ids\": ids[i:i + batch_size],\n",
        "                    \"embeddings\": embeddings_list[i:i + batch_size],\n",
        "                    \"documents\": [str(chunk) for chunk in chunks[i:i + batch_size]],\n",
        "                    \"metadatas\": safe_metadatas[i:i + batch_size]\n",
        "                }\n",
        "\n",
        "\n",
        "                self._validate_batch(batch)\n",
        "                self.collection.add(**batch)\n",
        "                pbar.update(len(batch[\"ids\"]))\n",
        "\n",
        "        print(f\"Successfully stored {len(ids)} embeddings\")\n",
        "\n",
        "    def _validate_batch(self, batch):\n",
        "        \"\"\"Strict validation for ChromaDB requirements\"\"\"\n",
        "        required_fields = [\"ids\", \"embeddings\", \"documents\"]\n",
        "        for field in required_fields:\n",
        "            if field not in batch:\n",
        "                raise ValueError(f\"Missing required field: {field}\")\n",
        "\n",
        "            if not isinstance(batch[field], list):\n",
        "                raise ValueError(f\"{field} must be a list\")\n",
        "\n",
        "\n",
        "        base_length = len(batch[\"ids\"])\n",
        "        for field in [\"embeddings\", \"documents\"]:\n",
        "            if len(batch[field]) != base_length:\n",
        "                raise ValueError(f\"Length mismatch: ids ({base_length}) vs {field} ({len(batch[field])})\")\n",
        "\n",
        "\n",
        "        if \"metadatas\" in batch:\n",
        "            if len(batch[\"metadatas\"]) != base_length:\n",
        "                raise ValueError(f\"Metadata length mismatch: expected {base_length}, got {len(batch['metadatas'])}\")\n",
        "\n",
        "            for meta in batch[\"metadatas\"]:\n",
        "                if not isinstance(meta, dict):\n",
        "                    raise ValueError(\"Each metadata must be a dictionary\")\n",
        "                for k, v in meta.items():\n",
        "                    if not isinstance(v, (str, int, float, bool)):\n",
        "                        raise ValueError(f\"Metadata value {k} must be str/int/float/bool, got {type(v)}\")\n",
        "\n",
        "    def generate_metadata(self, chunks):\n",
        "        metadatas = []\n",
        "        for chunk in chunks:\n",
        "            try:\n",
        "                chunk = str(chunk)\n",
        "\n",
        "\n",
        "                locations = [word for word in re.findall(r'\\b[A-Z][a-z]+\\b', chunk)\n",
        "                          if word not in ['India', 'Indian', 'The', 'You']\n",
        "                          and len(word) > 2][:3]\n",
        "                locations_str = \", \".join(locations)\n",
        "\n",
        "\n",
        "                activities = []\n",
        "                activity_map = {\n",
        "                    'heritage': ['fort', 'palace', 'museum', 'temple'],\n",
        "                    'adventure': ['trek', 'hike', 'camp', 'rafting'],\n",
        "                    'spiritual': ['ashram', 'meditation', 'yoga'],\n",
        "                    'beach': ['beach', 'sunset', 'wave', 'coast']\n",
        "                }\n",
        "                for activity, keywords in activity_map.items():\n",
        "                    if any(kw in chunk.lower() for kw in keywords):\n",
        "                        activities.append(activity)\n",
        "                activities_str = \", \".join(activities)\n",
        "\n",
        "                metadatas.append({\n",
        "                    \"locations\": locations_str,\n",
        "                    \"activities\": activities_str,\n",
        "                    \"word_count\": len(chunk.split())\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\" Metadata generation error: {str(e)[:100]}...\")\n",
        "                metadatas.append({\n",
        "                    \"locations\": \"\",\n",
        "                    \"activities\": \"\",\n",
        "                    \"word_count\": 0\n",
        "                })\n",
        "        return metadatas\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    chunks = np.load(\"/content/drive/MyDrive/Tourism/chunks.npy\", allow_pickle=True)\n",
        "    embeddings = np.load(\"/content/drive/MyDrive/Tourism/embeddings.npy\")\n",
        "\n",
        "\n",
        "\n",
        "    chroma_store = ChromaDBStore()\n",
        "\n",
        "\n",
        "    print(\"Generating ChromaDB-compatible metadata...\")\n",
        "    metadatas = chroma_store.generate_metadata(chunks)\n",
        "\n",
        "\n",
        "    try:\n",
        "        chroma_store.store_embeddings(chunks, embeddings, metadatas)\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error: {e}\")\n",
        "        print(\"Debugging tips:\")\n",
        "        print(\"- Check for None values in chunks\")\n",
        "        print(\"- Verify embeddings are finite numbers\")\n",
        "        print(\"- Inspect first 5 metadata items:\")\n",
        "        for i, meta in enumerate(metadatas[:5]):\n",
        "            print(f\"  {i}: {meta}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kOlkjtOkFu0",
        "outputId": "572221d1-0a42-4209-f74d-589b169e471e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6a8b7956-8081-40be-851e-9ae666e1fa50  chroma.sqlite3\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/MyDrive/vector/Culture_and_Quiz2_db\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "2AWpohj-lEE2",
        "outputId": "e0e556c5-cabd-4392-d8db-c14fdff5ab99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import shutil\\nfrom langchain.vectorstores import Chroma\\n\\n# === Vectorstore 1 ===\\ndrive_path1 = \"/content/drive/MyDrive/vector/Culture_and_Quiz2_db\"\\nlocal_path1 = \"/content/local_vectorstore1\"\\nshutil.copytree(drive_path1, local_path1)\\nvectorstore1 = Chroma(persist_directory=local_path1, embedding_function=embedding)\\n\\n# === Vectorstore 2 ===\\ndrive_path2 = \"/content/drive/MyDrive/vector/Indian_Stories_db\"\\nlocal_path2 = \"/content/local_vectorstore2\"\\nshutil.copytree(drive_path2, local_path2)\\nvectorstore2 = Chroma(persist_directory=local_path2, embedding_function=embedding)\\n\\n# === Vectorstore 3 ===\\ndrive_path3 = \"/content/drive/MyDrive/vector/Tourism_db\"\\nlocal_path3 = \"/content/local_vectorstore3\"\\nshutil.copytree(drive_path3, local_path3)\\nvectorstore3 = Chroma(persist_directory=local_path3, embedding_function=embedding)\\n\\nprint(\"âœ… All 3 vectorstores loaded successfully from local copies.\")'"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''import shutil\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# === Vectorstore 1 ===\n",
        "drive_path1 = \"/content/drive/MyDrive/vector/Culture_and_Quiz2_db\"\n",
        "local_path1 = \"/content/local_vectorstore1\"\n",
        "shutil.copytree(drive_path1, local_path1)\n",
        "vectorstore1 = Chroma(persist_directory=local_path1, embedding_function=embedding)\n",
        "\n",
        "# === Vectorstore 2 ===\n",
        "drive_path2 = \"/content/drive/MyDrive/vector/Indian_Stories_db\"\n",
        "local_path2 = \"/content/local_vectorstore2\"\n",
        "shutil.copytree(drive_path2, local_path2)\n",
        "vectorstore2 = Chroma(persist_directory=local_path2, embedding_function=embedding)\n",
        "\n",
        "# === Vectorstore 3 ===\n",
        "drive_path3 = \"/content/drive/MyDrive/vector/Tourism_db\"\n",
        "local_path3 = \"/content/local_vectorstore3\"\n",
        "shutil.copytree(drive_path3, local_path3)\n",
        "vectorstore3 = Chroma(persist_directory=local_path3, embedding_function=embedding)\n",
        "\n",
        "print(\"âœ… All 3 vectorstores loaded successfully from local copies.\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvLX7tJ_cSAx",
        "outputId": "30ae177d-c3db-491b-f349-bcf2572aba15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Vector DB loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Path to the existing DB\n",
        "persist_directory = \"/content/local_vectorstore1\"\n",
        "\n",
        "# Use same embedding model used during creation\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load the persisted vector store\n",
        "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "\n",
        "print(\"âœ… Vector DB loaded successfully.\")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vun59kxBcoXj",
        "outputId": "aced5ea1-4548-4458-b2cf-5fdb31fd1fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Story vector DB loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# === Configuration ===\n",
        "story_vector_db_path = \"/content/local_vectorstore2\"\n",
        "\n",
        "# === Load embedding model (same one used previously) ===\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# === Load the persisted vector store ===\n",
        "story_vectorstore = Chroma(persist_directory=story_vector_db_path, embedding_function=embedding)\n",
        "\n",
        "print(\"âœ… Story vector DB loaded successfully.\")\n",
        "\n",
        "story_retriever = story_vectorstore.as_retriever()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-liLWvQdqA9",
        "outputId": "0c31eba6-972c-401b-9e8b-b94402dd3ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded ChromaDB Tourism collection with 1673 items.\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "\n",
        "class ChromaDBTourismLoader:\n",
        "    def __init__(self, persist_dir=\"/content/local_vectorstore3\"):\n",
        "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
        "        self.collection = self.client.get_collection(name=\"indian_tourism\")\n",
        "\n",
        "    def get_collection(self):\n",
        "        return self.collection\n",
        "\n",
        "# Usage\n",
        "chroma_loader = ChromaDBTourismLoader()\n",
        "tourism_collection = chroma_loader.get_collection()\n",
        "\n",
        "print(f\"âœ… Loaded ChromaDB Tourism collection with {tourism_collection.count()} items.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgBsO3jGvWE",
        "outputId": "71dfc67e-b435-4fe3-e1d6-2aab2fce3471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amir Khusrau was a South Asian Sufi musician, poet, and scholar. He is also referred to as the \"Parrot of India\" and is regarded as the \"Father of Qawwali\".\n",
            "\n",
            "ðŸ”Ž Source 1:\n",
            "ðŸ“„ Metadata: {'title': '', 'page': 752, 'keywords': '', 'creationdate': '2021-01-31T19:27:20+05:30', 'total_pages': 786, 'page_label': '753', 'moddate': '2021-01-31T23:23:26+05:30', 'subject': '', 'producer': '', 'creator': '', 'source': '/content/drive/MyDrive/Culture_and_Quiz/ind_culture_studymaterial.pdf', 'author': ''}\n",
            "ðŸ“š Content Preview:\n",
            "book on â€œLaya Yogaâ€ and used to spend most of his time doing meditation.\n",
            "His followers led a warrior ascetic movement from 14th century onwards to\n",
            "resist against the harassment faced from Islamic and British rule. They also\n",
            "had an expertise in martial arts. The Goraknath Math is a monastery of the\n",
            "N...\n",
            "\n",
            "ðŸ”Ž Source 2:\n",
            "ðŸ“„ Metadata: {'title': '', 'producer': '', 'creationdate': '2021-01-31T19:27:20+05:30', 'page': 752, 'author': '', 'keywords': '', 'moddate': '2021-01-31T23:23:26+05:30', 'creator': '', 'subject': '', 'source': '/content/drive/MyDrive/Culture_and_Quiz/ind_culture_studymaterial.pdf', 'total_pages': 786, 'page_label': '753'}\n",
            "ðŸ“š Content Preview:\n",
            "book on â€œLaya Yogaâ€ and used to spend most of his time doing meditation.\n",
            "His followers led a warrior ascetic movement from 14th century onwards to\n",
            "resist against the harassment faced from Islamic and British rule. They also\n",
            "had an expertise in martial arts. The Goraknath Math is a monastery of the\n",
            "N...\n",
            "\n",
            "ðŸ”Ž Source 3:\n",
            "ðŸ“„ Metadata: {'author': 'STAFF', 'page_label': '105', 'page': 104, 'total_pages': 363, 'creator': 'MicrosoftÂ® Word 2016', 'creationdate': '2021-06-29T11:59:28+05:30', 'producer': 'MicrosoftÂ® Word 2016', 'source': '/content/drive/MyDrive/Culture_and_Quiz/indian_culture_heritage.pdf', 'moddate': '2021-06-29T11:59:28+05:30'}\n",
            "ðŸ“š Content Preview:\n",
            "Persian to explain mysticism. A number of Sufi works were also written in Bengali. The most \n",
            "notable writer of this period was Amir Khusrau (l 252 -1325) the follower of Nizamuddin Auliya. \n",
            "Khusrau took pride in being an Indian and looked at the history and culture of Hindustan as a part of \n",
            "his own...\n",
            "\n",
            "ðŸ”Ž Source 4:\n",
            "ðŸ“„ Metadata: {'source': '/content/drive/MyDrive/Culture_and_Quiz/indian_culture_heritage.pdf', 'producer': 'MicrosoftÂ® Word 2016', 'page_label': '105', 'creationdate': '2021-06-29T11:59:28+05:30', 'author': 'STAFF', 'creator': 'MicrosoftÂ® Word 2016', 'moddate': '2021-06-29T11:59:28+05:30', 'total_pages': 363, 'page': 104}\n",
            "ðŸ“š Content Preview:\n",
            "Persian to explain mysticism. A number of Sufi works were also written in Bengali. The most \n",
            "notable writer of this period was Amir Khusrau (l 252 -1325) the follower of Nizamuddin Auliya. \n",
            "Khusrau took pride in being an Indian and looked at the history and culture of Hindustan as a part of \n",
            "his own...\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 3. Define the prompt template\n",
        "template = \"\"\"You are a an expert on Indian culture and heritage answering questions based on the given context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# 4. Set up QA chain with the custom prompt\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "\n",
        "# 5. Ask a question\n",
        "query = \"What is Amir Khusrau?\"\n",
        "response = qa_chain.run(query)\n",
        "print(response)\n",
        "\n",
        "# Retrieve relevant documents\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "if not docs:\n",
        "    print(\"âš ï¸ No relevant documents found.\")\n",
        "else:\n",
        "    for i, doc in enumerate(docs):\n",
        "        print(f\"\\nðŸ”Ž Source {i+1}:\")\n",
        "        print(f\"ðŸ“„ Metadata: {doc.metadata}\")\n",
        "        print(f\"ðŸ“š Content Preview:\\n{doc.page_content[:300]}...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnG9ZoNQKDeu"
      },
      "outputs": [],
      "source": [
        "quiz_prompt_template = \"\"\"You are a quiz generator expert on Indian culture and heritage.\n",
        "\n",
        "{topic_instruction}\n",
        "\n",
        "Based on the context below, generate a quiz with {num_questions} multiple-choice questions.\n",
        "Each question should have four options and clearly indicate the correct answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Difficulty Level: {difficulty}\n",
        "\n",
        "Output Format:\n",
        "Q1. <question>\n",
        "A. <option1>\n",
        "B. <option2>\n",
        "C. <option3>\n",
        "D. <option4>\n",
        "Answer: <correct_option>\n",
        "\n",
        "Repeat for all questions.\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzQt0N9kRCJj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "story_prompt_template = \"\"\"You are a master storyteller specializing in Indian culture and heritage.\n",
        "\n",
        "Based on the following user request, generate a story that is engaging, culturally rich, and informative.\n",
        "\n",
        "User Request: {query}\n",
        "Desired Length: {length} words\n",
        "\n",
        "Use appropriate style, structure, and vocabulary for storytelling. Make it enjoyable while staying true to the cultural essence.\n",
        "\n",
        "Story:\n",
        "\"\"\"\n",
        "\n",
        "story_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"length\"],\n",
        "    template=story_prompt_template,\n",
        ")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "story_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=story_prompt\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCC9yq7tsXhg",
        "outputId": "7d589a23-fb6c-41a6-d98c-4d5e94d565a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a detailed travel itinerary for a 3-day trip to Kerala focusing on nature and backwaters:\n",
            "\n",
            "**DAY 1**\n",
            "------\n",
            "### Morning:\n",
            "Start your day with a leisurely boat ride through the tranquil backwaters of Kumarakom. Take in the breathtaking views of the lush green paddy fields, coconut groves, and bird sanctuaries as you glide through the waterways. (9:00 am - 11:00 am)\n",
            "### Afternoon:\n",
            "After the boat ride, head to the Kumarakom Bird Sanctuary, a haven for birdwatchers. Take a guided walk through the sanctuary and spot various species of birds, including migratory birds, kingfishers, and Siberian storks. (11:30 am - 1:30 pm)\n",
            "### Evening:\n",
            "In the evening, enjoy a sunset cruise on the backwaters, sipping on a refreshing drink and taking in the serene atmosphere. You can also try your hand at fishing or simply relax and soak in the tranquility of the surroundings. (4:00 pm - 6:30 pm)\n",
            "### Travel Time:\n",
            "Total travel time for the day: approximately 4 hours\n",
            "### Food Suggestions:\n",
            "For lunch, try some local Kerala cuisine at the Kumarakom Lake Resort's restaurant. For dinner, head to the nearby town of Kottayam and try some authentic Kerala dishes at the famous hotel, Hotel Sea Rock.\n",
            "### Cultural Tips:\n",
            "\n",
            "* Be sure to wear comfortable clothing and shoes for the boat ride and walk at the bird sanctuary.\n",
            "* Respect the local environment and wildlife by not littering and not disturbing the birds.\n",
            "* Try to learn some basic Malayalam phrases to interact with the locals.\n",
            "\n",
            "**DAY 2**\n",
            "------\n",
            "### Morning:\n",
            "Start the day with a visit to the Periyar Wildlife Sanctuary in Thekkady. Take a guided boat ride through the Periyar Lake and spot various species of wildlife, including elephants, monkeys, and birds. (9:00 am - 12:00 pm)\n",
            "### Afternoon:\n",
            "After the boat ride, head to the Spice Village, a resort that offers a range of activities, including a spice plantation tour and a traditional Kerala martial arts demonstration. (1:00 pm - 4:00 pm)\n",
            "### Evening:\n",
            "In the evening, enjoy a traditional Kerala dinner at the Spice Village's restaurant, featuring local delicacies such as sadya and karimeen pollichathu. (7:00 pm - 9:00 pm)\n",
            "### Travel Time:\n",
            "Total travel time for the day: approximately 6 hours\n",
            "### Food Suggestions:\n",
            "For lunch, try some local cuisine at the Spice Village's restaurant.\n",
            "### Cultural Tips:\n",
            "\n",
            "* Be respectful of the wildlife and the sanctuary's rules.\n",
            "* Try to learn some basic Malayalam phrases to interact with the locals.\n",
            "* Don't forget to bring sunscreen and a hat to protect yourself from the sun.\n",
            "\n",
            "**DAY 3**\n",
            "------\n",
            "### Morning:\n",
            "Start the day with a visit to the Marari Beach, a stunning beach located near Alleppey. Take a leisurely walk along the beach and enjoy the sunrise. (6:30 am - 8:30 am)\n",
            "### Afternoon:\n",
            "After breakfast, head to the Alleppey Backwater Village, a unique eco-friendly resort that offers a range of activities, including kayaking and rowing. (9:00 am - 12:30 pm)\n",
            "### Evening:\n",
            "In the evening, enjoy a traditional Kerala dinner at the resort's restaurant, featuring local delicacies such as idiyappam and parippu. (7:30 pm - 9:30 pm)\n",
            "### Travel Time:\n",
            "Total travel time for the day: approximately 4.5 hours\n",
            "### Food Suggestions:\n",
            "For lunch, try some local cuisine at the Alleppey Backwater Village's restaurant.\n",
            "### Cultural Tips:\n",
            "\n",
            "* Be respectful of the local environment and wildlife.\n",
            "* Try to learn some basic Malayalam phrases to interact with the locals.\n",
            "* Don't forget to bring sunscreen and a hat to protect yourself from the sun.\n",
            "\n",
            "This itinerary provides a mix of nature, culture, and relaxation, giving you a unique experience of Kerala's natural beauty and rich cultural heritage.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Define the itinerary prompt template\n",
        "itinerary_prompt_template = \"\"\"You are a travel expert specializing in Indian tourism.\n",
        "\n",
        "Based on the following user query, generate a detailed travel itinerary.\n",
        "\n",
        "Include:\n",
        "- Morning, afternoon, and evening activities\n",
        "- Travel time estimates\n",
        "- Recommended dining options\n",
        "- Cultural tips and precautions\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Format:\n",
        "DAY 1\n",
        "------\n",
        "- Morning:\n",
        "- Afternoon:\n",
        "- Evening:\n",
        "- Travel Time:\n",
        "- Food Suggestions:\n",
        "- Cultural Tips:\n",
        "\n",
        "(Repeat for each day as needed)\n",
        "\n",
        "Itinerary:\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "itinerary_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=itinerary_prompt_template,\n",
        ")\n",
        "\n",
        "# Create the LLM chain\n",
        "itinerary_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=itinerary_prompt\n",
        ")\n",
        "\n",
        "query = \"Plan a 3-day trip to Kerala focusing on nature and backwaters.\"\n",
        "response = itinerary_chain.run(query)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFM8j0-kH5cw"
      },
      "outputs": [],
      "source": [
        "# Function for QA\n",
        "def answer_question2(user_question):\n",
        "    try:\n",
        "        response = qa_chain.run(user_question)\n",
        "\n",
        "        # Optionally get documents for source display\n",
        "        docs = retriever.get_relevant_documents(user_question)\n",
        "        if not docs:\n",
        "            return f\"ðŸ§  Answer:\\n{response.strip()}\\n\\nâš ï¸ No relevant documents found.\"\n",
        "\n",
        "        sources = \"\\n\\n\".join(\n",
        "            [f\"ðŸ”Ž Source {i+1}:\\n{doc.page_content[:300]}...\" for i, doc in enumerate(docs)]\n",
        "        )\n",
        "\n",
        "        return f\"ðŸ§  Answer:\\n{response.strip()}\\n\\nðŸ“š Sources:\\n{sources}\"\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def generate_quiz(num_questions, difficulty, topic=\"\"):\n",
        "    try:\n",
        "        # Use topic if given, else use difficulty for retrieval\n",
        "        search_query = topic if topic else difficulty\n",
        "        docs = retriever.get_relevant_documents(search_query)\n",
        "        if not docs:\n",
        "            return \"âš ï¸ No relevant documents found.\"\n",
        "\n",
        "        # Build context from the top relevant documents\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs[:10]])\n",
        "\n",
        "        topic_instruction = f\"Generate a quiz specifically on the topic: {topic}.\" if topic else \"Generate a general quiz on Indian culture and heritage.\"\n",
        "\n",
        "        # Fill the prompt template\n",
        "        quiz_prompt = quiz_prompt_template.format(\n",
        "            context=context,\n",
        "            num_questions=num_questions,\n",
        "            difficulty=difficulty,\n",
        "            topic_instruction=topic_instruction,\n",
        "        )\n",
        "\n",
        "        # Get quiz from LLM\n",
        "        quiz_output = llm.predict(quiz_prompt)\n",
        "\n",
        "        return quiz_output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error: {str(e)}\"\n",
        "\n",
        "def generate_story(query, length):\n",
        "    try:\n",
        "        response = story_chain.run(query=query, length=length)\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "# --- Itinerary Generator Function using LLMChain ---\n",
        "def generate_itinerary(query):\n",
        "    try:\n",
        "        global itinerary_chain  # Make sure this chain is defined globally as shown before\n",
        "        if 'itinerary_chain' not in globals():\n",
        "            return \"âš ï¸ Itinerary chain not initialized.\"\n",
        "\n",
        "        response = itinerary_chain.run(query)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error generating itinerary: {str(e)}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvNcLUkkoRtW"
      },
      "outputs": [],
      "source": [
        "# Variables to hold quiz state\n",
        "quiz_questions = []\n",
        "user_score = 0\n",
        "current_index = 0\n",
        "hint_used = False\n",
        "\n",
        "import re\n",
        "\n",
        "def generate_semantic_hint(option_text):\n",
        "    try:\n",
        "        docs = retriever.get_relevant_documents(option_text)\n",
        "        if not docs:\n",
        "            return \"No hint available.\"\n",
        "\n",
        "        context = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
        "\n",
        "        hint_prompt = f\"\"\"\n",
        "You are a helpful assistant. Based on the following content, generate a hint that helps a user guess the correct answer in a quiz. Do NOT mention the answer or its letter. Make the hint informative and indirect.\n",
        "\n",
        "Content:\n",
        "{context}\n",
        "\n",
        "Hint:\"\"\"\n",
        "\n",
        "        return llm.predict(hint_prompt).strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating hint: {str(e)}\"\n",
        "\n",
        "\n",
        "def parse_quiz_output(quiz_text):\n",
        "    questions = []\n",
        "    blocks = quiz_text.strip().split(\"Q\")[1:]  # skip the first empty split\n",
        "    for block in blocks:\n",
        "        lines = block.strip().split(\"\\n\")\n",
        "        q_line = \"Q\" + lines[0]\n",
        "        options = [line for line in lines[1:] if line.strip().startswith((\"A.\", \"B.\", \"C.\", \"D.\"))]\n",
        "\n",
        "        # Extract answer line\n",
        "        answer_line = next((line for line in lines if \"Answer:\" in line), \"\")\n",
        "\n",
        "        # Use regex to extract only A/B/C/D from the answer line\n",
        "        match = re.search(r\"Answer\\s*[:\\-]?\\s*([A-D])\", answer_line, re.IGNORECASE)\n",
        "        answer = match.group(1).upper() if match else \"\"\n",
        "\n",
        "        # Get full option text to generate hint from\n",
        "        option_map = {opt[0]: opt[3:].strip() for opt in options if len(opt) > 2}\n",
        "        hint_text = generate_semantic_hint(option_map.get(answer, \"\"))\n",
        "\n",
        "        questions.append({\n",
        "            \"question\": q_line,\n",
        "            \"options\": options,\n",
        "            \"answer\": answer,\n",
        "            \"hint\": hint_text\n",
        "        })\n",
        "    return questions\n",
        "\n",
        "\n",
        "\n",
        "# Generate quiz and reset state\n",
        "def start_quiz(num_questions, difficulty, topic):\n",
        "    global quiz_questions, user_score, current_index, hint_used\n",
        "    quiz_text = generate_quiz(num_questions, difficulty, topic)\n",
        "    quiz_questions = parse_quiz_output(quiz_text)\n",
        "    user_score = 0\n",
        "    current_index = 0\n",
        "    hint_used = False\n",
        "    return show_question()\n",
        "\n",
        "# Show current question\n",
        "def show_question():\n",
        "    if current_index < len(quiz_questions):\n",
        "        q = quiz_questions[current_index]\n",
        "        return f\"{q['question']}\\n\" + \"\\n\".join(q['options']), \"\", \"\", f\"Score: {user_score}\"\n",
        "    else:\n",
        "        return \"ðŸŽ‰ Quiz Completed!\", \"\", \"\", f\"Final Score: {user_score}/{len(quiz_questions)}\"\n",
        "\n",
        "# Submit answer with feedback and correct answer option shown\n",
        "def submit_answer(user_answer):\n",
        "    global current_index, user_score, hint_used\n",
        "\n",
        "    user_input = user_answer.strip().upper()\n",
        "    if user_input not in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "        return show_question()[0], \"âš ï¸ Please enter only A, B, C, or D.\", \"\", f\"Score: {user_score}\", gr.update(visible=True)\n",
        "\n",
        "    q = quiz_questions[current_index]\n",
        "    correct_option = q[\"answer\"].strip().upper()\n",
        "    correct_text = next((opt for opt in q[\"options\"] if opt.startswith(correct_option)), \"\")\n",
        "\n",
        "    if user_input == correct_option:\n",
        "        user_score += 1\n",
        "        feedback = \"âœ… Correct!\"\n",
        "    else:\n",
        "        feedback = f\"âŒ Wrong! Correct answer: {correct_text}\"\n",
        "\n",
        "    hint_used = False\n",
        "    return show_question()[0], feedback, \"\", f\"Score: {user_score}\", gr.update(visible=True)\n",
        "\n",
        "\n",
        "# Function to go to next question\n",
        "def next_question():\n",
        "    global current_index\n",
        "    current_index += 1\n",
        "    return show_question()[0], \"\", \"\", f\"Score: {user_score}\", gr.update(visible=False)\n",
        "\n",
        "\n",
        "# Show hint\n",
        "def get_hint():\n",
        "    global hint_used\n",
        "    if not hint_used:\n",
        "        hint_used = True\n",
        "        current_question = quiz_questions[current_index]\n",
        "        debug_msg = f\"[DEBUG] Hint generated for: {current_question['answer']} - {current_question['hint']}\"\n",
        "        print(debug_msg)\n",
        "        return current_question[\"hint\"]\n",
        "    else:\n",
        "        return \"Hint already used for this question.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T81etW1zcSlW",
        "outputId": "1a6d7fa0-5493-4156-de26-6ca5623c6055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
            "Requirement already satisfied: indic-transliteration in /usr/local/lib/python3.11/dist-packages (2.3.69)\n",
            "Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2024.11.6)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.15.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.10.2)\n",
            "Requirement already satisfied: roman in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (8.1.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (4.13.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install deep-translator\n",
        "!pip install indic-transliteration\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNVBmYGqcMne",
        "outputId": "97e48374-bf0f-411e-9006-b360103f9c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "à²‰à²¤à³à²¤à²°:\n",
            "à²­à²¾à²°à²¤à³€à²¯ à²‡à²¤à²¿à²¹à²¾à²¸à²¦ à²¬à²—à³à²—à³† à²’à²‚à²¦à³ à²ªà³à²°à²¶à³à²¨à³†!\n",
            "\n",
            "à²•à³Šà²Ÿà³à²Ÿà²¿à²°à³à²µ à²¸à²¨à³à²¨à²¿à²µà³‡à²¶à²¦à²²à³à²²à²¿, à²®à³à²¹à²®à³à²®à²¦à²¨à³ à²†à²³à³à²µà²¿à²•à³†à²¯ à²•à³à²¸à²¿à²¤à²¦ à²®à³‡à²²à³† à²°à²¾à²œà²§à²¾à²¨à²¿ à²•à³Šà²³à³†à²¯à²¿à²¤à³ à²Žà²‚à²¦à³ à²‰à²²à³à²²à³‡à²–à²¿à²¸à²²à²¾à²—à²¿à²¦à³†. à²‡à²¦à³ à²®à³Šà²˜à²²à³ à²¸à²¾à²®à³à²°à²¾à²œà³à²¯à²¦ à²…à²µà²¨à²¤à²¿à²¯à²¨à³à²¨à³ à²¸à³‚à²šà²¿à²¸à³à²¤à³à²¤à²¦à³†, à²‡à²¦à³ 16 à²°à²¿à²‚à²¦ 18 à²¨à³‡ à²¶à²¤à²®à²¾à²¨à²—à²³à²²à³à²²à²¿ à²­à²¾à²°à²¤à²¦à²²à³à²²à²¿ à²†à²¡à²³à²¿à²¤ à²…à²§à²¿à²•à²¾à²°à²µà²¾à²—à²¿à²¤à³à²¤à³.\n",
            "\n",
            "à²†à²¦à³à²¦à²°à²¿à²‚à²¦, \"à²­à²¾à²°à²¤à²¦ à²°à²¾à²œà²§à²¾à²¨à²¿?\" à²Žà²‚à²¬ à²ªà³à²°à²¶à³à²¨à³†à²—à³† à²‰à²¤à³à²¤à²°? à²…à²¥à²µà²¾ \"à²­à²¾à²°à²¤à²¦ à²°à²¾à²œà²§à²¾à²¨à²¿ à²à²¨à³?\" \"à²®à³Šà²˜à²²à³ à²¸à²¾à²®à³à²°à²¾à²œà³à²¯à²µà³ à²•à³à²·à³€à²£à²¿à²¸à²¿à²¦à³à²¦à²°à²¿à²‚à²¦ à²®à²¤à³à²¤à³ à²µà²¿à²µà²¿à²§ à²ªà³à²°à²¦à³‡à²¶à²—à²³à²¨à³à²¨à³ à²µà²¿à²­à²¿à²¨à³à²¨ à²…à²§à²¿à²•à²¾à²°à²—à²³à²¿à²‚à²¦ à²†à²³à²²à²¾à²—à²¿à²¦à³†\" à²Žà²‚à²¦à³ \"à²† à²¸à²®à²¯à²¦à²²à³à²²à²¿ à²­à²¾à²°à²¤à²¦ à²’à²‚à²¦à³ à²¬à²‚à²¡à²µà²¾à²³ à²‡à²°à²²à²¿à²²à³à²².\"\n",
            "\n",
            "à²¹à³‡à²—à²¾à²¦à²°à³‚, à²¨à³€à²µà³ à²† à²¸à²®à²¯à²¦à²²à³à²²à²¿ à²­à²¾à²°à²¤à²¦ à²°à²¾à²œà²§à²¾à²¨à²¿à²¯ à²¬à²—à³à²—à³† à²•à³‡à²³à³à²¤à³à²¤à²¿à²¦à³à²¦à²°à³†, à²…à²¦à³ à²¨à³€à²µà³ à²‰à²²à³à²²à³‡à²–à²¿à²¸à³à²µ à²¨à²¿à²°à³à²¦à²¿à²·à³à²Ÿ à²…à²µà²§à²¿à²¯à²¨à³à²¨à³ à²…à²µà²²à²‚à²¬à²¿à²¸à²¿à²°à³à²¤à³à²¤à²¦à³†. à²®à³Šà²˜à²²à³ à²¸à²¾à²®à³à²°à²¾à²œà³à²¯à²¦ à²¸à²®à²¯à²¦à²²à³à²²à²¿, à²°à²¾à²œà²§à²¾à²¨à²¿ à²†à²—à³à²°à²¾ à²†à²—à²¿à²¤à³à²¤à³, à²†à²¦à²°à³† à²¨à²‚à²¤à²° à²…à²¦à²¨à³à²¨à³ à²¦à³†à²¹à²²à²¿à²—à³† à²¸à³à²¥à²³à²¾à²‚à²¤à²°à²¿à²¸à²²à²¾à²¯à²¿à²¤à³.\n",
            "\n",
            "ðŸ“š à²®à³‚à²²à²—à²³à³:\n",
            "ðŸ”Ž à²®à³‚à²² 1:\n",
            "FictlToToTraceONACOUNTOFTHEWANTONDESTRUCTIONOFARICTECTICTUREMENTONCESTONTONTONDESTRUCTIONODOFARICTECTICTICTORTSANDSANDBYTHEIRSUCCESSORS.WEENTHENTECATITLINTODECAYONTHEDEDECLINEOFMUHAMMADANRULE, GAURWASAUNTHEDEDECLINEDECLINEDECLINEOFMUMUHAMMADANULE\n",
            "\n",
            "ðŸ”Ž à²®à³‚à²² 2:\n",
            "FictlToToTraceONACOUNTOFTHEWANTONDESTRUCTIONOFARICTECTICTUREMENTONCESTONTONTONDESTRUCTIONODOFARICTECTICTICTORTSANDSANDBYTHEIRSUCCESSORS.WEENTHENTECATITLINTODECAYONTHEDEDECLINEOFMUHAMMADANRULE, GAURWASAUNTHEDEDECLINEDECLINEDECLINEOFMUMUHAMMADANULE\n",
            "\n",
            "ðŸ”Ž à²®à³‚à²² 3:\n",
            ".\n",
            "\n",
            "ðŸ”Ž à²®à³‚à²² 4:\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "def translate_text(text, target_lang=\"hi\"):\n",
        "    try:\n",
        "        return GoogleTranslator(source='auto', target=target_lang).translate(text)\n",
        "    except Exception as e:\n",
        "        print(f\"[DEBUG] Translation error: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "def answer_question(user_question, language_code=\"en\"):\n",
        "    try:\n",
        "        response = qa_chain.run(user_question)\n",
        "        docs = retriever.get_relevant_documents(user_question)\n",
        "\n",
        "        if not docs:\n",
        "            answer = f\"ðŸ§  Answer:\\n{response.strip()}\\n\\nâš ï¸ No relevant documents found.\"\n",
        "        else:\n",
        "            sources = \"\\n\\n\".join(\n",
        "                [f\"ðŸ”Ž Source {i+1}:\\n{doc.page_content[:300]}...\" for i, doc in enumerate(docs)]\n",
        "            )\n",
        "            answer = f\"ðŸ§  Answer:\\n{response.strip()}\\n\\nðŸ“š Sources:\\n{sources}\"\n",
        "\n",
        "        if language_code != \"en\":\n",
        "            answer = translate_text(answer, target_lang=language_code)\n",
        "            if docs:\n",
        "                sources_translated = \"\\n\\n\".join(\n",
        "                    [translate_text(doc.page_content[:300], target_lang=language_code) for doc in docs]\n",
        "                )\n",
        "                answer = answer.replace(sources, sources_translated)\n",
        "\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error: {str(e)}\"\n",
        "\n",
        "# Example\n",
        "answer = answer_question(\"à²­à²¾à²°à²¤à²¦ à²°à²¾à²œà²§à²¾à²¨à²¿ à²¯à²¾à²µà³à²¦à³?\", language_code=\"kn\")\n",
        "print(answer)\n",
        "\n",
        "# Define the Story Generation function with translation\n",
        "def generate_story_with_translation(request, length, language_code=\"en\"):\n",
        "    \"\"\"Generate the story and translate it to the desired language.\"\"\"\n",
        "    try:\n",
        "        # Generate the story using the prompt chain\n",
        "        story = story_chain.run(query=request, length=length)\n",
        "\n",
        "        # Translate the story if needed\n",
        "        if language_code != \"en\":\n",
        "            story = translate_text(story, target_lang=language_code)\n",
        "\n",
        "        return story\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cusezAj8agi"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "def speak_story(text, language_code=\"en\"):\n",
        "    try:\n",
        "        # gTTS supports: 'en' (English), 'hi' (Hindi), 'kn' (Kannada), etc.\n",
        "        tts = gTTS(text=text, lang=language_code)\n",
        "        filename = f\"story_{language_code}.mp3\"\n",
        "        tts.save(filename)\n",
        "        print(f\"[âœ… SUCCESS] Audio saved as {filename}\")\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"[âŒ ERROR] TTS generation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajLeAXoERcJh",
        "outputId": "a7bc68fe-093f-4aae-bd05-44ce5f56139e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.11/dist-packages (3.14.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TVYzMVCSDEi"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def voice_to_text(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    try:\n",
        "        with sr.AudioFile(audio_path) as source:\n",
        "            audio_data = recognizer.record(source)\n",
        "            text = recognizer.recognize_google(audio_data)\n",
        "            return text\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Sorry, I couldn't understand the audio.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nYtITI9DGX7k",
        "outputId": "53085d92-768e-429c-f21d-f5842ade0835"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!pip install gradio'"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''!pip install gradio'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "rRMVQEOmfqsg",
        "outputId": "ef86486d-dc07-4184-e4cc-35c8760ec8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://250e51f44bb255acb5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://250e51f44bb255acb5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Gradio UI\n",
        "\n",
        "# Create the Gradio app within a Blocks context\n",
        "with gr.Blocks(theme=\"hmb/amethyst\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ðŸ‡®ðŸ‡³ **Welcome to Incredible India!**\n",
        "    ðŸ¤– I'm your Cultural Buddy! Let's explore India together with:\n",
        "    - ðŸ“š Fun Facts & Questions\n",
        "    - ðŸ§  Exciting Quizzes\n",
        "    - ðŸ“– Magical Stories\n",
        "    - ðŸŒ Cool Travel Itineraries\n",
        "    ---\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    gr.HTML(\n",
        "    \"\"\"\n",
        "    <div style=\"text-align:center;\">\n",
        "      <iframe width=\"560\" height=\"315\"\n",
        "        src=\"https://www.youtube.com/embed/pL2mtjTagHI\"\n",
        "        frameborder=\"0\"\n",
        "        allowfullscreen>\n",
        "      </iframe>\n",
        "    </div>\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "    with gr.Tabs():\n",
        "      with gr.TabItem(\"ðŸ“š Ask Me Anything!\"):\n",
        "          qa_input = gr.Textbox(placeholder=\"e.g., What is the Taj Mahal?\", label=\"â“ Your Question\")\n",
        "          qa_button = gr.Button(\"ðŸŽ¯ Get Answer\")\n",
        "          qa_output = gr.Textbox(label=\"ðŸ“– My Answer\")\n",
        "\n",
        "          # Language selector dropdown for translation\n",
        "          language_selector = gr.Dropdown(choices=[\"en\", \"hi\", \"kn\"], label=\"Select Language\", value=\"en\")\n",
        "\n",
        "          # Voice input components\n",
        "          qa_audio = gr.Audio(label=\"ðŸŽ¤ Speak Your Question\", type=\"filepath\", format=\"wav\")\n",
        "          qa_speech_button = gr.Button(\"ðŸ—£ï¸ Convert Voice to Text\")\n",
        "\n",
        "          # Click handlers for both buttons\n",
        "          qa_button.click(fn=answer_question, inputs=[qa_input, language_selector], outputs=qa_output)\n",
        "          qa_speech_button.click(fn=voice_to_text, inputs=qa_audio, outputs=qa_input)\n",
        "\n",
        "\n",
        "      with gr.TabItem(\"ðŸ§  Take a Quiz\"):\n",
        "          gr.Markdown(\"### ðŸŽ® Interactive Quiz Game on Indian Culture\")\n",
        "\n",
        "          num_q = gr.Number(value=5, label=\"Number of Questions\")\n",
        "          diff = gr.Radio([\"easy\", \"medium\", \"hard\"], label=\"Difficulty\")\n",
        "          topic = gr.Textbox(label=\"Topic (optional)\", placeholder=\"e.g. Dance, Festivals\")\n",
        "\n",
        "          start_btn = gr.Button(\"ðŸš€ Start Quiz\")\n",
        "          question_display = gr.Textbox(label=\"ðŸ“– Question\", lines=5)\n",
        "          user_input = gr.Textbox(label=\"ðŸ“ Your Answer (A/B/C/D)\")\n",
        "          feedback_display = gr.Textbox(label=\"âœ… Feedback\")\n",
        "          score_display = gr.Textbox(label=\"ðŸ… Score\", interactive=False)\n",
        "          hint_output = gr.Textbox(label=\"ðŸ§© Hint\")\n",
        "\n",
        "          submit_btn = gr.Button(\"ðŸŽ¯ Submit Answer\")\n",
        "          next_btn = gr.Button(\"âž¡ï¸ Next\", visible=False)\n",
        "          hint_btn = gr.Button(\"ðŸ’¡ Hint\")\n",
        "\n",
        "          # Start quiz\n",
        "          start_btn.click(\n",
        "              fn=start_quiz,\n",
        "              inputs=[num_q, diff, topic],\n",
        "              outputs=[question_display, feedback_display, hint_output, score_display]\n",
        "          )\n",
        "\n",
        "          # Submit answer\n",
        "          submit_btn.click(\n",
        "              fn=submit_answer,\n",
        "              inputs=[user_input],\n",
        "              outputs=[question_display, feedback_display, hint_output, score_display, next_btn]\n",
        "          )\n",
        "\n",
        "          # Show next question\n",
        "          next_btn.click(\n",
        "              fn=next_question,\n",
        "              inputs=[],\n",
        "              outputs=[question_display, feedback_display, hint_output, score_display, next_btn]\n",
        "          )\n",
        "\n",
        "          # Show hint\n",
        "          hint_btn.click(\n",
        "              fn=get_hint,\n",
        "              inputs=[],\n",
        "              outputs=[hint_output]\n",
        "          )\n",
        "\n",
        "\n",
        "      with gr.TabItem(\"ðŸ“– Storytelling\"):\n",
        "            story_input = gr.Textbox(label=\"ðŸŒŸ Your Request\", placeholder=\"Tell me a story about Mythology\")\n",
        "            length_input = gr.Slider(minimum=50, maximum=500, step=50, label=\"Desired Length\", value=150)\n",
        "            language_selector = gr.Dropdown(choices=[\"en\", \"hi\", \"kn\"], label=\"Select Language\", value=\"en\")\n",
        "\n",
        "            generate_story_btn = gr.Button(\"ðŸ“š Generate Story\")\n",
        "            story_output = gr.Textbox(label=\"ðŸ“ Story\", lines=10)\n",
        "            read_btn = gr.Button(\"ðŸ”Š Read Story\")\n",
        "            audio_output = gr.Audio(label=\"ðŸŽ§ Listen to Story\", interactive=False)\n",
        "\n",
        "            generate_story_btn.click(fn=generate_story_with_translation,\n",
        "                                      inputs=[story_input, length_input, language_selector],\n",
        "                                      outputs=story_output)\n",
        "            read_btn.click(fn=speak_story, inputs=[story_output], outputs=[audio_output])\n",
        "\n",
        "\n",
        "      with gr.TabItem(\"ðŸŒ Travel Planner!\"):\n",
        "          travel_input = gr.Textbox(label=\"ðŸžï¸ Where do you want to go?\", placeholder=\"e.g., Best places to visit in Rajasthan\")\n",
        "          travel_button = gr.Button(\"ðŸŒ Plan My Trip!\")\n",
        "          travel_output = gr.Textbox(label=\"ðŸ—ºï¸ Itinerary\")\n",
        "          travel_button.click(fn=generate_itinerary, inputs=travel_input, outputs=travel_output)\n",
        "\n",
        "gr.Markdown(\"---\")\n",
        "gr.Markdown(\"ðŸ‘¦ðŸ‘§ Made for young explorers! Let's celebrate our heritage and have fun learning! ðŸ‡®ðŸ‡³\")\n",
        "\n",
        "# Launch safely in Colab\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    demo.launch(share=True)\n",
        "else:\n",
        "    demo.launch(server_port=7860)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8g50gKVZt3HW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}